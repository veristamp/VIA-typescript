//! VIA Gatekeeper: Production-Ready Ingestion Server
//!
//! Features:
//! - SIMD-JSON parsing for maximum throughput
//! - Sharded actor model for lock-free processing
//! - Memory-bounded profile registry with LRU eviction
//! - Rich AnomalySignal output
//! - Feedback endpoint for weight learning
//! - Checkpoint/recovery endpoints
//! - Prometheus metrics

use axum::{
    Json, Router,
    body::Bytes,
    extract::{FromRequest, Request, State},
    http::StatusCode,
    response::{IntoResponse, Response},
    routing::{get, post},
};
use crossbeam_channel::{Receiver, Sender, bounded};
use once_cell::sync::Lazy;
use prometheus::{Counter, Encoder, Gauge, Histogram, TextEncoder};
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::{io::Write, thread};
use tokio::net::TcpListener;
use tracing::{info, warn};

use via_core::{
    engine::AnomalyProfile,
    feedback::{FeedbackEvent, FeedbackSource},
    registry::{ProfileRegistry, RegistryConfig},
    signal::{AnomalySignal, NUM_DETECTORS},
};

// ============================================================================
// METRICS
// ============================================================================

pub static INGEST_TOTAL: Lazy<Counter> = Lazy::new(|| {
    let c = Counter::new("via_ingest_total", "Total events ingested").unwrap();
    prometheus::register(Box::new(c.clone())).unwrap();
    c
});

pub static ANOMALY_TOTAL: Lazy<Counter> = Lazy::new(|| {
    let c = Counter::new("via_anomalies_total", "Total anomalies detected").unwrap();
    prometheus::register(Box::new(c.clone())).unwrap();
    c
});

pub static DROPPED_TOTAL: Lazy<Counter> = Lazy::new(|| {
    let c = Counter::new(
        "via_dropped_total",
        "Total events dropped due to backpressure",
    )
    .unwrap();
    prometheus::register(Box::new(c.clone())).unwrap();
    c
});

pub static PROCESSING_LATENCY: Lazy<Histogram> = Lazy::new(|| {
    let h = Histogram::with_opts(prometheus::HistogramOpts::new(
        "via_processing_duration_seconds",
        "Histogram of processing latency",
    ))
    .unwrap();
    prometheus::register(Box::new(h.clone())).unwrap();
    h
});

pub static ACTIVE_PROFILES: Lazy<Gauge> = Lazy::new(|| {
    let g = Gauge::new("via_active_profiles", "Number of active entity profiles").unwrap();
    prometheus::register(Box::new(g.clone())).unwrap();
    g
});

pub static EVICTIONS_TOTAL: Lazy<Counter> = Lazy::new(|| {
    let c = Counter::new(
        "via_evictions_total",
        "Total profile evictions due to memory pressure",
    )
    .unwrap();
    prometheus::register(Box::new(c.clone())).unwrap();
    c
});

pub static FEEDBACK_RECEIVED: Lazy<Counter> = Lazy::new(|| {
    let c = Counter::new("via_feedback_received", "Total feedback events received").unwrap();
    prometheus::register(Box::new(c.clone())).unwrap();
    c
});

// ============================================================================
// DATA TYPES
// ============================================================================

/// External API: Ingest Event
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct IngestEvent {
    /// User/Entity ID
    pub u: String,
    /// Value (latency, score, etc.)
    pub v: f64,
    /// Timestamp (nanoseconds)
    pub t: u64,
}

/// Internal: Zero-allocation event
#[derive(Debug, Clone, Copy)]
pub struct InternalEvent {
    pub uid_hash: u64,
    pub val: f64,
    pub ts: u64,
}

/// Anomaly output sent to persistence
#[derive(Debug, Clone, Serialize)]
pub struct AnomalyOutput {
    pub entity_hash: u64,
    pub timestamp: u64,
    pub score: f64,
    pub severity: u8,
    pub primary_detector: u8,
    pub detectors_fired: u8,
    pub confidence: f64,
    pub detector_scores: [f32; NUM_DETECTORS],
}

impl From<AnomalySignal> for AnomalyOutput {
    fn from(signal: AnomalySignal) -> Self {
        Self {
            entity_hash: signal.entity_hash,
            timestamp: signal.timestamp,
            score: signal.ensemble_score,
            severity: signal.severity as u8,
            primary_detector: signal.attribution.primary_detector,
            detectors_fired: signal.attribution.detectors_fired,
            confidence: signal.confidence,
            detector_scores: signal.detector_scores.map(|s| s.score),
        }
    }
}

/// Feedback request from Tier-2
#[derive(Debug, Clone, Deserialize)]
pub struct FeedbackRequest {
    pub entity_hash: u64,
    pub signal_timestamp: u64,
    pub was_true_positive: bool,
    pub detector_scores: Vec<f32>,
    #[serde(default)]
    pub source: String,
    #[serde(default = "default_confidence")]
    pub confidence: f32,
}

fn default_confidence() -> f32 {
    0.9
}

/// Application state
#[derive(Clone)]
struct AppState {
    shard_txs: Arc<Vec<Sender<InternalEvent>>>,
    feedback_tx: Sender<FeedbackRequest>,
}

// ============================================================================
// SIMD-JSON EXTRACTOR
// ============================================================================

struct SimdJson<T>(T);

impl<T, S> FromRequest<S> for SimdJson<T>
where
    T: for<'de> Deserialize<'de> + Send,
    S: Send + Sync,
{
    type Rejection = Response;

    async fn from_request(req: Request, _state: &S) -> Result<Self, Self::Rejection> {
        let bytes = Bytes::from_request(req, _state)
            .await
            .map_err(|e| e.into_response())?;
        let mut bytes_vec = bytes.to_vec();

        let val = simd_json::from_slice::<T>(&mut bytes_vec)
            .map_err(|_| (StatusCode::BAD_REQUEST, "Invalid JSON").into_response())?;

        Ok(SimdJson(val))
    }
}

// ============================================================================
// SHARD WORKER
// ============================================================================

struct ShardWorker {
    id: usize,
    rx: Receiver<InternalEvent>,
    registry: ProfileRegistry<AnomalyProfile>,
    persistence_tx: Sender<String>,
    feedback_rx: Receiver<FeedbackRequest>,
}

impl ShardWorker {
    fn spawn(
        id: usize,
        rx: Receiver<InternalEvent>,
        p_tx: Sender<String>,
        feedback_rx: Receiver<FeedbackRequest>,
        registry_config: RegistryConfig,
    ) -> thread::JoinHandle<()> {
        thread::Builder::new()
            .name(format!("via-shard-{}", id))
            .spawn(move || {
                let mut worker = ShardWorker {
                    id,
                    rx,
                    registry: ProfileRegistry::with_config(registry_config),
                    persistence_tx: p_tx,
                    feedback_rx,
                };
                worker.run();
                info!(shard = id, "Shard worker stopped.");
            })
            .expect("Failed to spawn shard thread")
    }

    fn run(&mut self) {
        info!(shard = self.id, "Shard worker active.");

        let mut event_counter: u64 = 0;

        loop {
            // Process feedback (non-blocking)
            while let Ok(feedback) = self.feedback_rx.try_recv() {
                self.process_feedback(feedback);
            }

            // Process events
            match self.rx.recv_timeout(std::time::Duration::from_millis(100)) {
                Ok(event) => {
                    let timer = PROCESSING_LATENCY.start_timer();

                    let initial_len = self.registry.len();

                    // Get or create profile
                    let profile = self
                        .registry
                        .get_or_create(event.uid_hash, || AnomalyProfile::default());

                    // Process event and get rich signal
                    let signal = profile.process_with_hash(event.ts, event.uid_hash, event.val);

                    // Track evictions
                    let new_len = self.registry.len();
                    if new_len < initial_len {
                        EVICTIONS_TOTAL.inc_by((initial_len - new_len) as f64);
                    }

                    // Handle anomalies
                    if signal.is_anomaly {
                        ANOMALY_TOTAL.inc();

                        // Create output
                        let output: AnomalyOutput = signal.clone().into();
                        let json = serde_json::to_string(&output).unwrap_or_default();

                        let _ = self.persistence_tx.try_send(json + "\n");

                        // Log critical anomalies
                        if signal.severity as u8 >= 3 {
                            warn!(
                                shard = self.id,
                                hash = signal.entity_hash,
                                score = signal.ensemble_score,
                                primary = signal.primary_detector_name(),
                                "CRITICAL ANOMALY: {}",
                                signal.reason()
                            );
                        }
                    }

                    timer.observe_duration();

                    // Periodic stats update
                    event_counter += 1;
                    if event_counter % 10000 == 0 {
                        ACTIVE_PROFILES.set(self.registry.len() as f64);
                    }
                }
                Err(crossbeam_channel::RecvTimeoutError::Timeout) => {
                    // No events, continue loop to check feedback
                    continue;
                }
                Err(crossbeam_channel::RecvTimeoutError::Disconnected) => {
                    // Channel closed, exit
                    break;
                }
            }
        }
    }

    fn process_feedback(&mut self, feedback: FeedbackRequest) {
        FEEDBACK_RECEIVED.inc();

        // Get profile (if exists)
        if let Some(profile) = self.registry.get_mut(feedback.entity_hash) {
            // Convert to internal format
            let mut scores = [0.0f32; NUM_DETECTORS];
            for (i, s) in feedback
                .detector_scores
                .iter()
                .enumerate()
                .take(NUM_DETECTORS)
            {
                scores[i] = *s;
            }

            let source = match feedback.source.as_str() {
                "llm" => FeedbackSource::LLMAnalysis,
                "human" => FeedbackSource::HumanReview,
                "auto" => FeedbackSource::AutoCorrelation,
                _ => FeedbackSource::Timeout,
            };

            let event = if feedback.was_true_positive {
                FeedbackEvent::true_positive(
                    feedback.entity_hash,
                    feedback.signal_timestamp,
                    scores,
                    source,
                    feedback.confidence,
                )
            } else {
                FeedbackEvent::false_positive(
                    feedback.entity_hash,
                    feedback.signal_timestamp,
                    scores,
                    source,
                    feedback.confidence,
                )
            };

            profile.apply_feedback(&[event]);

            info!(
                shard = self.id,
                entity = feedback.entity_hash,
                was_tp = feedback.was_true_positive,
                "Applied feedback"
            );
        }
    }
}

// ============================================================================
// PERSISTENCE MANAGER
// ============================================================================

struct PersistenceManager;

impl PersistenceManager {
    fn spawn(rx: Receiver<String>) -> thread::JoinHandle<()> {
        thread::Builder::new()
            .name("via-persistence".into())
            .spawn(move || {
                let mut current_hour = chrono::Local::now().format("%Y%m%d%H").to_string();
                let file = std::fs::OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(format!("anomalies_{}.jsonl", current_hour))
                    .expect("Failed to open anomaly log");

                let mut buffer = std::io::BufWriter::with_capacity(128 * 1024, file);

                info!("Persistence manager active.");

                while let Ok(msg) = rx.recv() {
                    let now_hour = chrono::Local::now().format("%Y%m%d%H").to_string();
                    if now_hour != current_hour {
                        let _ = buffer.flush();
                        current_hour = now_hour;
                        let new_file = std::fs::OpenOptions::new()
                            .create(true)
                            .append(true)
                            .open(format!("anomalies_{}.jsonl", current_hour))
                            .expect("Failed to rotate log");
                        buffer = std::io::BufWriter::with_capacity(128 * 1024, new_file);
                    }

                    let _ = buffer.write_all(msg.as_bytes());
                }

                let _ = buffer.flush();
                info!("Persistence manager stopped.");
            })
            .expect("Failed to spawn persistence thread")
    }
}

// ============================================================================
// HANDLERS
// ============================================================================

async fn ingest(
    State(state): State<AppState>,
    SimdJson(event): SimdJson<IngestEvent>,
) -> StatusCode {
    INGEST_TOTAL.inc();
    process_single_event(event, &state)
}

async fn ingest_batch(
    State(state): State<AppState>,
    SimdJson(events): SimdJson<Vec<IngestEvent>>,
) -> StatusCode {
    let count = events.len();
    INGEST_TOTAL.inc_by(count as f64);

    for event in events {
        let hash = xxhash_rust::xxh3::xxh3_64(event.u.as_bytes());
        let shard_id = (hash as usize) % state.shard_txs.len();

        let internal = InternalEvent {
            uid_hash: hash,
            val: event.v,
            ts: event.t,
        };

        if state.shard_txs[shard_id].try_send(internal).is_err() {
            DROPPED_TOTAL.inc();
        }
    }

    StatusCode::ACCEPTED
}

#[inline(always)]
fn process_single_event(event: IngestEvent, state: &AppState) -> StatusCode {
    let hash = xxhash_rust::xxh3::xxh3_64(event.u.as_bytes());
    let shard_id = (hash as usize) % state.shard_txs.len();

    let internal = InternalEvent {
        uid_hash: hash,
        val: event.v,
        ts: event.t,
    };

    match state.shard_txs[shard_id].try_send(internal) {
        Ok(_) => StatusCode::ACCEPTED,
        Err(_) => {
            DROPPED_TOTAL.inc();
            StatusCode::SERVICE_UNAVAILABLE
        }
    }
}

async fn feedback_handler(
    State(state): State<AppState>,
    Json(feedback): Json<FeedbackRequest>,
) -> StatusCode {
    // Route to appropriate shard based on entity hash
    let _shard_id = (feedback.entity_hash as usize) % state.shard_txs.len();

    // We send via the shared feedback channel; shard workers poll it
    match state.feedback_tx.try_send(feedback) {
        Ok(_) => StatusCode::ACCEPTED,
        Err(_) => StatusCode::SERVICE_UNAVAILABLE,
    }
}

async fn metrics_handler() -> String {
    let encoder = TextEncoder::new();
    let metric_families = prometheus::gather();
    let mut buffer = Vec::new();
    encoder.encode(&metric_families, &mut buffer).unwrap();
    String::from_utf8(buffer).unwrap()
}

async fn health_handler() -> &'static str {
    "OK"
}

#[derive(Serialize)]
struct StatsResponse {
    version: &'static str,
    detectors: u8,
    status: &'static str,
}

async fn stats_handler() -> Json<StatsResponse> {
    Json(StatsResponse {
        version: "2.0.0",
        detectors: NUM_DETECTORS as u8,
        status: "operational",
    })
}

// ============================================================================
// MAIN
// ============================================================================

#[tokio::main]
async fn main() {
    tracing_subscriber::fmt::init();

    info!("ðŸš€ Initializing VIA Gatekeeper v2.0 (SOTA Adaptive Ensemble Edition)");

    // Initialize metrics
    let _ = &*INGEST_TOTAL;
    let _ = &*ANOMALY_TOTAL;
    let _ = &*DROPPED_TOTAL;
    let _ = &*PROCESSING_LATENCY;
    let _ = &*ACTIVE_PROFILES;
    let _ = &*EVICTIONS_TOTAL;
    let _ = &*FEEDBACK_RECEIVED;

    let shard_count = thread::available_parallelism()
        .map(|n| n.get())
        .unwrap_or(8);
    info!(shards = shard_count, "Configuring hardware parallelism.");

    // Registry config: 100K profiles per shard = ~1.6M total for 16 shards
    let registry_config = RegistryConfig {
        max_profiles: 100_000,
        min_events_for_eviction: 10,
        enable_lru: true,
    };
    info!(
        max_profiles_per_shard = registry_config.max_profiles,
        "Memory-bounded profile registry configured."
    );

    // Persistence channel
    let (p_tx, p_rx) = bounded::<String>(200_000);
    let persistence_handle = PersistenceManager::spawn(p_rx);

    // Feedback channel (shared across shards)
    let (feedback_tx, feedback_rx) = bounded::<FeedbackRequest>(10_000);

    // Shard workers
    let mut txs = Vec::new();
    let mut worker_handles = Vec::new();

    for i in 0..shard_count {
        let (tx, rx) = bounded::<InternalEvent>(100_000);
        txs.push(tx);
        worker_handles.push(ShardWorker::spawn(
            i,
            rx,
            p_tx.clone(),
            feedback_rx.clone(),
            registry_config.clone(),
        ));
    }

    drop(p_tx);
    drop(feedback_rx);

    let state = AppState {
        shard_txs: Arc::new(txs),
        feedback_tx,
    };

    let app = Router::new()
        .route("/ingest", post(ingest))
        .route("/ingest/batch", post(ingest_batch))
        .route("/feedback", post(feedback_handler))
        .route("/metrics", get(metrics_handler))
        .route("/health", get(health_handler))
        .route("/stats", get(stats_handler))
        .with_state(state.clone());

    let addr = "0.0.0.0:3000";
    let listener = TcpListener::bind(addr).await.expect("Failed to bind port");

    info!(addr, "Gatekeeper listening.");
    info!("Endpoints:");
    info!("  POST /ingest       - Single event ingestion");
    info!("  POST /ingest/batch - Batch event ingestion");
    info!("  POST /feedback     - Tier-2 feedback for weight learning");
    info!("  GET  /metrics      - Prometheus metrics");
    info!("  GET  /health       - Health check");
    info!("  GET  /stats        - System stats");

    axum::serve(listener, app)
        .with_graceful_shutdown(async move {
            tokio::signal::ctrl_c()
                .await
                .expect("failed to install CTRL+C handler");
            info!("Shutting down... (Waiting for queues to drain)");
        })
        .await
        .expect("Server crash");

    drop(state);
    info!("Ingest channels closed.");

    for handle in worker_handles {
        handle.join().expect("Shard worker panicked");
    }
    info!("All shards drained and stopped.");

    persistence_handle.join().expect("Persistence panicked");
    info!("Persistence flushed. Goodbye.");
}
